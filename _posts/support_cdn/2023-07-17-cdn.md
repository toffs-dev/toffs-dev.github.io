---
layout: post
title: What is a CDN ?
categories: [Support,Cdn]
---
### What is a CDN?
A content delivery network (CDN) refers to a network of servers strategically placed in various locations worldwide. Its primary function is to store and distribute content in close proximity to end users. By caching HTML pages, JavaScript files, stylesheets, images, videos, and other assets necessary for loading Internet content, CDNs enable swift content delivery.

CDNs have witnessed a remarkable surge in popularity, with the majority of web traffic now being served through them. Prominent websites like Facebook, Netflix, and Amazon rely on CDNs to efficiently handle their traffic. Moreover, a well-configured CDN can also act as a defense against common malicious attacks, such as Distributed Denial of Service (DDoS) attacks, providing an added layer of protection for websites.
### Is a CDN the same as a web host?
Although a CDN cannot serve as a content host or replace the necessity for appropriate web hosting, it plays a vital role in caching content at the network edge, thereby enhancing website performance. Numerous websites encounter difficulties in meeting their performance requirements with conventional hosting services, prompting them to choose CDNs instead.

CDNs are widely preferred due to their ability to minimize hosting bandwidth through caching, prevent service interruptions, and enhance security. By alleviating some of the significant challenges associated with traditional web hosting, CDNs have become a popular choice.

### What are the benefits of using a CDN?
While the advantages of utilizing a CDN may vary based on the size and requirements of an internet property, they can generally be categorized into four distinct components:

Enhanced website load times: By leveraging a nearby CDN server and implementing various optimizations, content can be distributed closer to website visitors. This results in faster page loading times, reducing bounce rates and encouraging visitors to spend more time on the site. In essence, a faster website translates to increased visitor retention and engagement.

Reduced bandwidth costs: Bandwidth consumption expenses for website hosting are a significant cost factor. CDNs employ caching and other optimization techniques to decrease the amount of data that needs to be provided by the origin server. Consequently, hosting costs for website owners are reduced.

Improved content availability and redundancy: Traffic surges or hardware failures can disrupt the normal functioning of a website. Due to their distributed nature, CDNs are capable of handling high volumes of traffic and are more resilient against hardware failures compared to individual origin servers. This ensures continuous availability of content and improves overall website reliability.

Enhanced website security: CDNs contribute to bolstering website security through various means, including DDoS mitigation, improved security certificates, and other optimization measures. By incorporating these security features, CDNs help safeguard websites against malicious attacks and enhance overall protection.

In summary, CDNs offer benefits such as faster website loading times, reduced bandwidth costs, improved content availability, and enhanced website security. These advantages make CDNs an attractive choice for website owners, irrespective of their website's size and requirements.

### How does a CDN work?
A CDN, at its essence, is a network of interconnected servers designed to efficiently deliver content with speed, cost-effectiveness, reliability, and security in mind. To ensure faster connectivity, CDNs strategically position servers at exchange points where different networks intersect.

These exchange points, known as Internet exchange points (IXPs), serve as central locations where various internet service providers connect with one another to facilitate the exchange of traffic generated on their respective networks. By establishing connections at these high-speed and extensively interconnected locations, CDN providers can minimize costs and transit times associated with delivering data at high speeds.


In addition to server placement at IXPs, CDNs employ several optimizations to enhance standard client/server data transfers. They strategically position data centers in key locations worldwide, bolster security measures, and employ designs that can withstand various types of failures and internet congestion. This comprehensive approach ensures efficient content delivery across the globe.

Latency - How does a CDN improve website load times?
Websites lose users quickly if they load content slowly. CDN services offer various methods to reduce load times:

Minimizing Distance: CDNs have a distributed network across the globe, bringing website resources closer to users. Instead of connecting to a website's origin server, users can connect to a nearby data center. This reduces travel time and improves service speed.

Optimizing Hardware and Software: CDNs employ hardware and software optimizations like efficient load balancing and solid-state hard drives. These enhancements enable faster data delivery to users, improving overall performance.

Data Reduction: CDNs reduce data transfer by employing tactics like minification and file compression. They decrease file sizes, resulting in faster load times. By sending smaller files, CDNs enhance the efficiency of content delivery.

TLS/SSL Optimization: CDNs can accelerate websites that utilize TLS/SSL certificates by optimizing connection reuse and enabling TLS false start. These optimizations help establish secure connections more quickly, further enhancing site speed.

Explore all the ways a CDN helps websites load faster

Reliability and redundancy - How does a CDN keep a website always online?
Ensuring continuous uptime is crucial for individuals or organizations with an online presence. The occurrence of hardware failures or sudden increases in web traffic, whether caused by malicious attacks or a surge in popularity, can potentially lead to a web server going offline and preventing users from accessing a website or service. A comprehensive content delivery network (CDN) incorporates several features that effectively reduce the occurrence of downtime:

Load balancing: By evenly distributing network traffic across multiple servers, load balancing simplifies the process of scaling up to handle sudden spikes in traffic efficiently.

Intelligent failover: In the event that one or more CDN servers experience hardware malfunctions and go offline, intelligent failover ensures uninterrupted service. It redistributes the traffic to other operational servers, guaranteeing seamless accessibility.

Anycast routing: If technical difficulties affect an entire data center, Anycast routing swiftly transfers the traffic to an alternative available data center. This mechanism guarantees that users do not lose access to the website even during data center issues.

To delve deeper into how a CDN contributes to maintaining website availability, learn more about its functionalities.

### Data security - How does a CDN protect data?
CDNs play a crucial role in maintaining information security. They offer a reliable means to secure websites by providing up-to-date TLS/SSL certificates, thereby ensuring a robust level of authentication, encryption, and data integrity. It is important to delve into the security considerations associated with CDNs and explore effective measures for securely delivering content. Let's delve into the realm of CDN SSL/TLS security to gain a comprehensive understanding.
Bandwidth expense - How does a CDN reduce bandwidth costs?
Each request to an origin server consumes bandwidth whenever it responds. Discover how a CDN such as TOFFS minimizes origin requests and lowers expenses related to bandwidth usage.









CDN glossary
Anycast
### What is Anycast?
Anycast represents a network addressing and routing technique that enables incoming requests to be directed to multiple diverse locations or "nodes." Within a content delivery network (CDN), Anycast commonly directs incoming traffic to the closest data center equipped to handle the request effectively. By employing selective routing, an Anycast network demonstrates resilience against high traffic loads, network congestion, and DDoS attacks.

How does Anycast work?
Anycast network routing efficiently directs incoming connection requests to multiple data centers. When requests are sent to a single IP address linked to the Anycast network, the network distributes the data using a prioritization method. The selection process for determining the appropriate data center is usually optimized to minimize latency by choosing the data center closest to the requester. Anycast is known for its ability to establish a one-to-one association with multiple destinations and is considered one of the five primary network protocol methods employed in the Internet protocol.
Why use an Anycast network?
When multiple requests are made simultaneously to a single origin server, it can be overwhelmed by the high volume of traffic, leading to inefficient response times and potential service disruptions. However, by utilizing an Anycast network, the distribution of this load can be optimized. In an Anycast network, the traffic is not solely directed to one origin server but can be distributed across multiple data centers. Each of these data centers houses servers equipped to process and respond to incoming requests effectively. This routing approach ensures that the capacity of the origin server is not exceeded and minimizes service interruptions for clients accessing content from the origin server.
What is the difference between Anycast and Unicast?
The majority of the Internet operates through a routing method known as Unicast. In Unicast, each node on the network is assigned a unique IP address. Unicast is commonly used in home and office networks. However, if a computer connected to a wireless network receives a message indicating that the IP address is already in use, it means there is an IP address conflict occurring because another computer on the same Unicast network is already utilizing the same IP address. In most cases, this is not allowed.



When a Content Delivery Network (CDN) employs a Unicast address, traffic is directly routed to a specific node. This can create a vulnerability when the network experiences a significant surge in traffic, such as during a Distributed Denial of Service (DDoS) attack. Since the traffic is directed straight to a particular data center, it is possible for the location or its supporting infrastructure to become overwhelmed by the traffic, potentially resulting in denial-of-service for legitimate requests.

On the other hand, utilizing Anycast offers a highly resilient network. With Anycast, traffic is directed to the optimal path, allowing for automatic redirection of traffic to a nearby data center even if an entire data center goes offline. This enhances the network's ability to withstand disruptions and maintain the flow of traffic effectively.

How does an Anycast network mitigate a DDoS attack?
Anycast is an effective tool for mitigating Distributed Denial-of-Service (DDoS) attacks. Once other DDoS mitigation tools have filtered out some of the attack traffic, Anycast comes into play by distributing the remaining attack traffic across multiple data centers. This distribution prevents any single location from being overwhelmed with requests. If the capacity of the Anycast network surpasses that of the attack traffic, the attack is effectively mitigated.

DDoS attacks often utilize compromised computers, known as "zombies" or "bots," to form a botnet. These machines are spread across the internet and can generate an excessive amount of traffic, overwhelming a typical machine connected through Unicast.



A properly implemented Anycasted Content Delivery Network (CDN) enhances the network's surface area, allowing each data center of the CDN to absorb unfiltered denial-of-service traffic from a distributed botnet. As the network expands in size and capacity, launching an effective DDoS attack against anyone using the CDN becomes increasingly challenging.

However, establishing a true Anycasted network is not a simple task. It requires a CDN provider to maintain their own network hardware, establish direct relationships with upstream carriers, and fine-tune networking routes to prevent traffic disruptions between multiple locations. Toffs, for example, utilizes Anycast to achieve load balancing without traditional load balancers. For a more detailed understanding, you can refer to Toffs's blog post on their Anycast implementation.

Data center
### What is a data center?
A data center serves as a dedicated facility where numerous interconnected computers collaborate to handle, store, and exchange data. Data centers play a pivotal role for leading technology companies, serving as a vital element in delivering online services efficiently.
What is the difference between a data center and a point-of-presence (PoP)?
The terms data center and point-of-presence (PoP) are sometimes used interchangeably, although there are distinctions between them. Generally speaking, a PoP may denote a company's single server presence in a specific location, whereas a data center typically encompasses a facility that houses multiple servers. Toffs, instead of using multiple PoPs in one location, refers to a data center as a location where many of their servers are maintained.



The concept of a point-of-presence gained prominence during the court-ordered breakup of the Bell telephone system. In that legal decision, a point-of-presence referred to a location where long-distance carriers terminated services and redirected connections to a local network. Similarly, in the context of the modern Internet, a PoP commonly refers to a physical presence of content delivery networks (CDNs) in a particular location, often at the intersections between networks known as Internet exchange points (IxPs).

A data center, on the other hand, denotes a physical facility where computers are interconnected to enhance usability and reduce costs associated with storage, bandwidth, and other networking components. Data centers, including IxP co-location facilities, enable various Internet service providers, CDNs, and other infrastructure companies to connect and exchange traffic with one another. This fosters collaboration and resource sharing among these entities.
What are the common concerns in the design of a data center?
Creating a modern data center involves careful consideration of numerous components and factors. By implementing effective planning, maintenance, and security measures, the risk of downtime and data breaches can be significantly reduced.

Key considerations for a data center include:

Redundancy/Backup: The level of redundancy in a data center varies based on its quality. High-tier data centers incorporate multiple redundancies in power supply and backup servers within their infrastructure.

Efficiency: Large data centers consume a substantial amount of electricity, comparable to that of a small town. To minimize costs, data centers strive to optimize cooling processes and utilize energy-efficient hardware wherever possible.

Security: Data centers prioritize physical security to mitigate risks associated with unauthorized access attempts. This includes electronic surveillance, access controls, and on-site security guards.

Environmental Controls/Factors: Maintaining suitable environmental conditions is crucial for the optimal functioning of electronic hardware. Proper temperature and humidity control necessitate a balanced approach involving air conditioning, humidity regulation, and airflow management. In seismically active areas, securing servers against earthquakes becomes an additional concern.

Maintenance and Monitoring: On-site or on-call network engineers play a vital role in promptly addressing server crashes and hardware failures. Timely response ensures server uptime and minimizes disruptions to the quality of service.

Bandwidth: Adequate bandwidth is essential for a fully functional data center capable of handling the required network traffic. Bandwidth considerations play a central role in the design of data center infrastructure, encompassing both external network connections and internal data center topology.

Discover more about Toffs CDN, which boasts worldwide data centers strategically located across the globe.
Origin server
### What is an origin server?
What is the difference between an origin server and a CDN edge server?
Can an origin server still be attacked while using a CDN?
### Aching?
What is an origin server?
An origin server serves the purpose of processing and responding to incoming Internet requests from clients. It is commonly used in conjunction with edge servers or caching servers. Essentially, an origin server is a computer that runs one or more programs, specifically designed to receive and handle incoming Internet requests. It can handle the responsibility of delivering content for an Internet property, like a website, as long as the server can handle the incoming traffic without exceeding its processing capabilities, and latency is not a primary concern.

When a client makes a request to an origin server, the physical distance between them introduces latency, causing a delay in loading internet resources such as webpages. Additionally, establishing a secure Internet connection using SSL/TLS adds further latency due to the extra round-trip time (RTT) required between the client and the origin server. This directly affects the client's experience when requesting data from the origin. However, by utilizing a content delivery network (CDN), both the round-trip time and the number of requests to the origin server can be reduced, leading to decreased latency.
What is the difference between an origin server and a CDN edge server?
To put it in simpler terms, CDN edge servers are strategically placed computers located at key points between major Internet providers worldwide. Their purpose is to deliver content as quickly as possible. An edge server resides within a CDN at the "edge" of a network and is optimized for rapid request processing. By strategically positioning edge servers within Internet exchange points (IxPs) that connect networks, CDNs can minimize the time it takes to access specific locations on the Internet.

The primary function of these edge servers is to cache content, reducing the workload on one or more origin servers. By storing static assets like images, HTML, JavaScript files, and potentially other content closer to the requesting client machine, an edge server's cache significantly decreases the loading time of web resources. Origin servers still play a crucial role in CDNs, particularly for server-side code such as the database containing hashed client credentials used for authentication. Origin servers maintain this critical data.

Let's consider a simple example to illustrate how an edge server and an origin server collaborate to serve a login page and enable user authentication. A basic login page requires several static assets to be downloaded for proper webpage rendering:

HTML file for the webpage
CSS file for webpage styling
Multiple image files
Several JavaScript libraries
These files are static and identical for all website visitors. Consequently, they can be cached and served to clients directly from the edge server. This approach enables loading these files closer to the client machine, without consuming origin server bandwidth.



Afterward, when a user enters their login credentials and clicks "login," the request for dynamic content is sent back to the edge server. The edge server acts as a proxy and forwards the request to the origin server. The origin server verifies the user's identity using the associated database table and returns the specific account information.



This interaction between edge servers handling static content and origin servers providing dynamic content is a typical division of responsibilities in CDN usage. Some CDNs may offer capabilities that go beyond this basic model.

Can an origin server still be attacked while using a CDN?
In short, yes, a CDN can provide significant benefits. While it doesn't make an origin server invincible, when utilized effectively, it can make an origin server essentially invisible by acting as a protective shield against incoming requests. An essential aspect of CDN setup is concealing the real IP address of the origin server. To ensure maximum security, it is advisable for the CDN provider to recommend changing the IP address of the origin server during the implementation of a CDN strategy. This measure helps prevent DDoS attacks from bypassing the shield and directly targeting the origin server. Toffs's CDN offers comprehensive DDoS protection as part of its service.
Edge server
What is a CDN edge server?
An edge server within a CDN refers to a computer strategically positioned at the outermost periphery or "edge" of a network. It frequently acts as the intermediary between distinct networks, facilitating seamless connectivity. The primary objective of a CDN edge server is to store content in proximity to client machines that request it, resulting in decreased latency and enhanced page loading speed.

Functioning as an edge device, an edge server serves as a gateway into a network, alongside routers and routing switches. These edge devices are commonly deployed within Internet exchange points (IxPs) to enable interconnection and transit sharing among diverse networks.

How does an edge server work?
In a given network setup, various devices establish connections with each other following specific network patterns. When a network needs to connect to another network or the broader Internet, it requires a bridge to enable traffic flow between different locations. The hardware devices responsible for creating this bridge at the network's edge are referred to as edge devices.

Networks are interconnected at the edge, typically seen in home or office networks where multiple devices like mobile phones or computers connect and disconnect through a hub-and-spoke network model. All these devices exist within the same local area network (LAN) and connect to a central router, which facilitates their communication with each other.

To establish a connection between two networks, the networks must be linked at some point. The device enabling the connection between networks is known as an edge device.



Consider a scenario where a computer in Network A needs to connect to a computer in Network B. The connection must traverse from Network A, across the network edge, and into the second network. This principle also applies to more complex scenarios, such as connecting across the Internet. The ability of networks to share data is dependent on the presence of edge devices between them.

When a connection needs to traverse the Internet, additional intermediary steps are involved between Network A and Network B. For simplicity, imagine each network as a circle, and the point where the circles touch represents the network edge. To move a connection across the Internet, it typically encounters multiple networks and passes through several network edge nodes. Generally, the farther the connection travels, the more networks it needs to traverse. It may pass through different Internet service providers and backbone infrastructure hardware before reaching its destination.



CDN (Content Delivery Network) providers strategically deploy servers in numerous locations, with particular emphasis on the edge points between different networks. These edge servers establish connections with multiple networks, enabling efficient and rapid traffic flow between them. Without a CDN, transit may follow a slower or more convoluted path from source to destination. In the worst-case scenarios, traffic may unnecessarily travel long distances, even when connecting to a device across the street, resulting in unnecessary delays. By placing edge servers in key locations, CDNs can swiftly deliver content to users within various networks. To learn more about the benefits of using a CDN, you can explore how CDN performance works.

What is the difference between an edge server and an origin server?
The origin server is the central web server that receives all Internet traffic when a web property does not utilize a content delivery network (CDN). When a website relies solely on an origin server without a CDN, every Internet request must travel back to the physical location of that server, regardless of the user's location. Consequently, this leads to longer load times, especially for clients located far away from the server.

CDN edge servers, on the other hand, store and cache content in strategically positioned locations. By placing static assets like images, HTML, JavaScript files, and other content as close as possible to the user's machine, an edge server cache significantly reduces the time it takes to load web resources. This distribution of content helps alleviate the load on one or more origin servers. However, origin servers remain crucial when employing a CDN, as they handle essential server-side tasks such as managing a database of hashed client credentials used for authentication. To learn more about Toffs CDN and its globally distributed edge servers, explore their services.
Internet exchange point (IXP)
What is an Internet exchange point?
An Internet exchange point (IXP) serves as a physical hub where Internet infrastructure companies, including Internet Service Providers (ISPs) and CDNs, interconnect. These strategic points are positioned at the periphery of various networks, enabling network providers to exchange transit with other networks. Being present at an IXP location allows companies to optimize their connection routes by accessing transit from participating networks, leading to reduced latency, improved round-trip time, and potential cost savings.
How does an Internet exchange point work?
An Internet Exchange Point (IXP) essentially consists of one or more physical locations equipped with network switches that facilitate the routing of traffic among different member networks. By utilizing various methods, these networks collectively share the expenses associated with maintaining the physical infrastructure and related services. In a similar manner to how charges are incurred when transporting goods through intermediary locations like the Panama Canal, the transmission of traffic across diverse networks often incurs fees for delivery. To avoid these costs and other disadvantages linked to routing their traffic through third-party networks, member companies establish connections with each other through IXPs, thereby reducing expenses and minimizing latency.

IXPs are essentially large Local Area Networks (LANs) operating at Layer 2 of the OSI network model. They are constructed using one or multiple interconnected Ethernet switches deployed across one or more physical buildings. In essence, an IXP shares a basic conceptual framework with a home network, with the primary distinction being scale. IXPs can handle traffic ranging from hundreds of Megabits per second to several Terabits per second. Irrespective of size, their primary objective is to ensure efficient and seamless connectivity among multiple network routers. In contrast, a typical home network would usually have a single router serving several computers or mobile devices.

Over the past two decades, network interconnections have witnessed significant growth, paralleling the tremendous expansion of the global Internet. This expansion has led to the establishment of new data center facilities designed to house network equipment. Some of these data centers have attracted a substantial number of networks, largely due to the flourishing Internet exchange points operating within them.

Why are Internet exchange points important?
IXPs are essential for efficient internet traffic routing. Without them, traffic between networks would often have to rely on transit providers as intermediaries to transmit data from the source to the destination. While this is acceptable for international internet traffic due to the impracticality of maintaining direct connections with every ISP worldwide, it can negatively impact performance when relying on a backbone ISP to handle local traffic. This is because the backbone carrier may route data to a distant network in another city, leading to a phenomenon called trombone. In the worst-case scenario, traffic originating from one city and destined for an ISP in the same city would unnecessarily traverse long distances before being exchanged and returning. However, by incorporating IXPs into a CDN's infrastructure, the network can optimize data flow paths and eliminate inefficient routes, resulting in improved performance.


BGP, the Internetâ€™s backbone protocol
Networks communicate with each other through the utilization of the Border Gateway Protocol (BGP). This advanced protocol facilitates the clear separation of internal necessities and network-edge configurations within each network. BGP is employed for all peering activities at Internet Exchange Points (IXPs).
How do providers share traffic across different networks?
Transit
Transit refers to the agreement between a customer and its upstream provider, where the provider offers complete connectivity to the entire Internet. This service is paid for by the customer. The BGP protocol is employed to allow the customer's IP addresses to be announced to the transit provider and subsequently propagated throughout the global Internet.

Peering
Peering is the mechanism by which networks exchange IP addresses directly without the involvement of intermediaries. At Internet exchange points, there is typically no cost associated with transferring data between member networks. When data is transferred freely from one network to another, it is referred to as settlement-free peering.

Peering vs Paid Transit
However, some networks incur costs when transferring data. For instance, larger networks with relatively equal market share may engage in peering with other large networks but charge smaller networks for the peering service. Within a single Internet Exchange Point (IXP), a member company may have different arrangements with multiple other members. In such cases, companies may configure their routing protocols, specifically the BGP protocol, to optimize for reduced costs or reduced latency.

Depeering
Over time, relationships between networks may change, leading to the discontinuation of free interconnection. When a network decides to terminate its peering arrangement, it undergoes a process called depeering. Depeering can occur due to various reasons, such as imbalanced traffic ratios or when a network decides to charge the other party for interconnection. This process can be emotionally charged, and a network that feels spurned may deliberately disrupt the traffic of the other party after terminating the peering relationship.

How do IXPs use BGP?
Within an Internet Exchange Point (IXP), different providers can establish one-to-one connections using the BGP protocol. This protocol enables disparate networks to announce their IP addresses to each other, including the IP addresses they have provided connectivity to downstream (i.e., their customers). Once two networks establish a BGP session, they exchange their respective routes, facilitating direct traffic flow between them.

IXP or PNI Interconnection
In certain cases, two networks may consider their traffic to be crucial and decide to move from the shared infrastructure of an IXP to a dedicated interconnection between themselves. This dedicated connection, known as a Private Network Interconnect (PNI), is typically implemented using dark fiber and directly links a port on network A with a port on network B. The BGP configuration for PNI is nearly identical to that of a shared IXP peering setup.


CDN SSL TLS security
### What are the security risks to a CDN?
CDNs, like any networks connected to the Internet, need to protect themselves against on-path attacks, data breaches, and attempts to overload the targeted origin server through DDoS attacks. To safeguard against these vulnerabilities, CDNs employ various strategies, such as implementing robust SSL/TLS encryption and utilizing specialized encryption hardware.
What is SSL/TLS encryption?
Transport Layer Security (TLS) is a crucial protocol used to encrypt data transmitted over the Internet. It emerged as an improvement over Secure Sockets Layer (SSL), the initial widely adopted web encryption protocol, addressing the security vulnerabilities of its predecessor. While the industry sometimes uses the terms interchangeably due to historical reasons, any website starting with https:// instead of http:// employs TLS/SSL to establish secure communication between a browser and a server.

In order to safeguard valuable information, proper encryption practices are essential. The Internet's architecture involves data being transmitted through numerous locations, making it susceptible to interception. By employing a cryptographic protocol, TLS ensures that only the intended recipient can decipher and access the information, preventing intermediaries from deciphering the content of the transmitted data.

The TLS protocol encompasses three key components:

Authentication: It provides the capability to verify the authenticity and validity of the provided identifications.

Encryption: TLS enables the obfuscation of information transmitted between hosts, ensuring that it remains confidential and inaccessible to unauthorized individuals.

Integrity: The protocol facilitates the detection of forgery and tampering attempts, ensuring the integrity of the transmitted data is maintained.

### What is an SSL certificate?
In order to enable TLS (Transport Layer Security), a website requires an SSL certificate and a corresponding key. SSL certificates are files containing information about the site owner and the public part of an asymmetric key pair. These certificates are digitally signed by a certificate authority (CA) to ensure their accuracy. By trusting the certificate, users trust that the certificate authority has performed the necessary verification.



Operating systems and browsers usually have a predefined list of trusted certificate authorities. If a website presents a certificate signed by an untrusted CA, the browser alerts the visitor about a potential issue.

Certificates can be independently evaluated based on their strength, protocol support, and other characteristics. Ratings may change over time as better implementations become available or as factors affect the overall security of a certificate. If an origin server has an outdated or lower-grade SSL security implementation, it will receive a lower grade and may be susceptible to compromise.

Using a content delivery network (CDN) offers the added advantage of providing security to visitors accessing websites hosted within the CDN's network. Since visitors connect only to the CDN, the use of an older or less secure certificate between the origin server and the CDN does not impact the client's experience.



Nevertheless, relying on weaker server-to-edge security still poses a vulnerability and should be avoided. Upgrading the security of an origin server by utilizing free origin encryption is relatively simple and advisable.



Proper security also plays a significant role in organic search rankings, as encrypted websites tend to achieve higher rankings on Google searches.

An SSL/TLS connection operates differently than a traditional TCP/IP connection. Once the initial TCP connection is established, a separate exchange occurs to set up the secure connection. In this article, we refer to the device initiating the secure connection as the client and the device providing the secure connection as the server, as is the case when a user loads a webpage encrypted with SSL/TLS.

The TCP/IP handshake consists of three steps:

The client sends a SYN packet to the server to initiate the connection.
The server responds with a SYN/ACK packet to acknowledge the communication.
The client sends an ACK packet to acknowledge the receipt of the server's packet. After this sequence, the TCP connection is open for data transmission.



Once the TCP/IP handshake is complete, the TLS encryption handshake begins. The detailed processes involved in a TLS handshake implementation go beyond the scope of this guide. However, we will focus on the primary purpose of the handshake and the time required to complete the process.

At a high level, the TLS handshake comprises three main components:

Negotiating TLS versions and the cryptographic cipher to be used for communication.
Ensuring mutual authentication between the client and server.
Exchanging a key to be used for future encrypted communications.
The diagram below illustrates the steps involved in both the TCP/IP handshake and the TLS handshake. Each arrow represents a separate communication between the client and server. As TLS encryption involves more message exchanges, web page load times increase.


For illustrative purposes, the TCP handshake typically takes around 50ms, while the TLS handshake may take approximately 110ms. These durations mainly result from the time it takes for data to travel between the client and server in both directions. Round-trip time (RTT) quantifies the time required for information to travel from one device to another and back, representing the "cost" of establishing a connection. If left unoptimized and without the use of a CDN, additional RTT leads to increased latency and longer load times for end-users. Fortunately, there are optimizations available to enhance overall load time and reduce the number of round trips.

# How can SSL latency be improved?
Optimizing SSL can enhance page load time and reduce Round Trip Time (RTT). Below are three ways to optimize a TLS connection:

CDN Session Resumption: A content delivery network (CDN) can prolong the connection between the origin server and the CDN network by resuming the same session for subsequent requests. By keeping the connection alive, the CDN eliminates the need for time-consuming renegotiations between the CDN and the origin server when the client requires an uncached origin fetch. As long as additional requests are directed to the origin server while the connection to the CDN is maintained, visitors to the website will benefit from reduced latency.



Compared to a full TLS handshake, the cost of session resumption is less than 50%. This is primarily due to session resumption requiring only one round-trip, whereas a full TLS handshake necessitates two. Moreover, session resumption does not involve significant computational tasks such as large finite field arithmetic, which is required in new sessions. Consequently, the CPU cost for the client during session resumption is almost negligible in comparison to a full TLS handshake. For mobile users, the performance enhancements brought about by session resumption translate to a more responsive and battery-efficient browsing experience.


Enable TLS False Start:  When a user visits a website for the first time, the session resumption feature mentioned earlier may not provide any benefits. However, with TLS False Start, the sender can transmit application data without completing a full TLS handshake.



It's important to note that TLS False Start doesn't alter the TLS protocol itself; it simply adjusts the timing of data transfer. Once the client initiates the key exchange, encryption is ensured, and the transfer of data commences. By implementing this modification, the overall number of round trips is reduced, resulting in a significant reduction of approximately 60ms in latency.


Zero Round Trip Time Resumption (0-RTT): 0-RTT enables session resumption without adding any additional RTT latency to the connection. This improvement is applicable to resumed connections using TLS 1.3. With 0-RTT, the connection speed is enhanced, leading to a faster and smoother web experience for users who frequently visit specific websites. This speed boost is particularly noticeable when using mobile networks.

While 0-RTT is an effective enhancement, it does involve certain security tradeoffs. To mitigate the risk of replay attacks, a CDN service may implement restrictions on the type of HTTP requests and allowed parameters. For more detailed information, you can explore an introduction to 0-RTT.

# CDN protection from DDoS attacks
One of the significant security risks faced by web properties in today's internet landscape is the widespread use of distributed denial-of-service (DDoS) attacks. These attacks have evolved over time, becoming larger and more sophisticated. Attackers now employ botnets to direct overwhelming amounts of malicious traffic toward targeted websites. However, an effective countermeasure against DDoS attacks is the implementation of a robust and well-configured Content Delivery Network (CDN). By leveraging a CDN with numerous data center locations and substantial bandwidth capabilities, it becomes possible to withstand and mitigate a significant volume of attack traffic that would otherwise overpower the original server.

To enhance the security of a TLS connection, there are additional measures that can be taken. Discover more about the Toffs CDN and how it helps to address TLS attacks. Ensure that your website is utilizing HTTPS correctly by utilizing the Toffs Diagnostic Center to conduct a thorough check.


